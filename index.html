<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async="" src="./files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-WYV42QGLZ8');
  </script>

  <meta name="author" content="Zhengkai Jiang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Zhengkai Jiang's Homepage.">
  <title>Zhengkai Jiang</title>

  <link rel="stylesheet" href="./files/font.css">
  <link rel="stylesheet" href="./files/main.css">
  <script src="./files/main.js"></script>
  <script src="./files/scroll.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.993.0" data-gr-ext-installed="">
    <div class="outercontainer"> 
      <header>
        <div class="container header">
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#home">Home</a></div>
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#publications">Publications</a></div>
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#education">Education</a></div>
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#academic competitions">Academic Competitions</a></div> 
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#honors & awards">Honors & Awards</a></div>
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#professional activities">Professional Activities</a></div> 
        </div>
      </header>
      <div class="container body">
        <div class="content heading anchor" id="home" data-scroll-id="home" tabindex="-1" style="outline: none;">
          <div class="text info">
            <h1>Zhengkai Jiang</h1>
            <p>
            </p>
            <div>Researcher</div>
            <div>Tencent Youtu Lab</div>
            <div>Email:&nbsp;zhengkjiang [at] tencent (dot) com</div>
            <p>
            <span><a href="https://scholar.google.com/citations?user=ooBQi6EAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a></span> / 
            <span><a href="https://github.com/jiangzhengkai">Github</a></span> / 
            <span><a href="https://www.zhihu.com/people/zhengkaijiang">Zhihu</a></span> /
            <span><a href="./files/zhengkaijiang_resume.pdf">Resume</a></span>
            </p>
            <p>
            </p>
          </div>
          <div class="img"><img class="avatar" src="./imgs/me.jpg" alt="Photo"></div>
          <div class="text info">
            <p>I am currently a Researcher at <a href="https://ai.qq.com/hr/youtu.shtml">Tencent Youtu Lab</a>. My research interests are in computer vision and deep learning.
               <br><br>
               Previously, I obtained the master's degree at 2020 from <a href="http://www.nlpr.ia.ac.cn">National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</a>. 
               I received the bachelor's degree at 2017 from the Department of Automation, <a href="http://english.neu.edu.cn/">Northeastern University</a> with honors.
            </p>
          </div>
        </div>

        <div class="content" style="z-index:1;position:relative">
          <div class="text">
            <h3 style="margin-bottom:.5em">News</h3>
            <ul style="padding-bottom:1em">
              <li><strong>[09/2020]</strong> Two papers are accepted by <a href="https://nips.cc/" target="_blank">NeurIPS 2020</a>.</li>
              <li><strong>[07/2020]</strong> One paper is accepted by <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a>.</li>
              <li><strong>[03/2019]</strong> One paper is accepted by <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a>. </li>
              <li><strong>[11/2018]</strong> One paper is accepted by <a href="https://www.aaai.org/" target="_blank">AAAI 2019</a>. </li>
            </ul>
          </div>
        </div>

        <div class="content anchor" id="publications">
          <div class="text" style="z-index:1;position:relative">
            <h3 style="margin-bottom:0em">
              Publications
            </h3>
          </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/autoassign2020.jpg" alt="autoassign"></div>
              <div class="text">
                <div class="title">AutoAssign: Differentiable Label Assignment for Dense Object Detection</div> 
                <div class="authors">
                  <span class="author">Benjin Zhu</span>,
                  <span class="author">Jianfeng Wang</span>,
                  <span class="author jw">Zhengkai Jiang</span>, 
                  <span class="author">Fuhang Zong</span>, 
                  <span class="author">Songtao Liu</span>, 
                  <span class="author">Zeming Li</span>,
                  <span class="author">Jian Sun</span>
                </div>
                <div>
                  <span class="venue">Arxiv 2020</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2007.03496.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://megvii-basedetection/AutoAssign">Code</a></span>
                </div>
                <br>
                <div>
                  Performing differentiable label assignment for dense object detection.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/dynamichead.jpg" alt="dynamichead"></div>
              <div class="text">
                <div class="title">Fine-Grained Dynamic Head for Object Detection</div> 
                <div class="authors">
                  <span class="author">Lin Song</span>,
                  <span class="author">Yanwei Li</span>,
                  <span class="author jw">Zhengkai Jiang</span>, 
                  <span class="author">Zeming Li</span>, 
                  <span class="author">Hongbin Sun</span>,
                  <span class="author">Jian Sun</span>,
                  <span class="author">Nanning Zheng</span>
                </div>
                <div>
                  <span class="venue">NeurIPS 2020</span> /
                  <span class="tag"><a href="https://papers.nips.cc/paper/2020/file/7f6caf1f0ba788cd7953d817724c2b6e-Paper.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/StevenGrove/DynamicHead">Code</a></span>
                </div>
                <br>
                <div>
                  A fine-grained dynamic head is proposed to conditionally select a pixel-level combination of FPN features from different scales.
                </div>
              </div>
            </div>


            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/ltf-v2.jpg" alt="ltf-v2"></div>
              <div class="text">
                <div class="title">Rethinking Learnable Tree Filter for Generic Feature Transform</div> 
                <div class="authors">
                  <span class="author">Lin Song</span>,
                  <span class="author">Yanwei Li</span>,
                  <span class="author jw">Zhengkai Jiang</span>, 
                  <span class="author">Zeming Li</span>, 
                  <span class="author">Xiangyu Zhang</span>, 
                  <span class="author">Hongbin Sun</span>,
                  <span class="author">Jian Sun</span>,
                  <span class="author">Nanning Zheng</span>
                </div>
                <div>
                  <span class="venue">NeurIPS 2020</span> /
                  <span class="tag"><a href="https://papers.nips.cc/paper/2020/file/2952351097998ac1240cb2ab7333a3d2-Paper.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/StevenGrove/LearnableTreeFilterV2">Code</a></span>
                </div>
                <br>
                <div>
                  Performing differentiable tree-filter with a learnable unary term for generic feature transform.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/eccv2020.jpg" alt="lsts"></div>
              <div class="text">
                <div class="title">Learning Where to Focus for Efficient Video Object Detection</div> 
                <div class="authors">
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Yu Liu</span>,
                  <span class="author jw">Ceyuan Yang</span>,
                  <span class="author">Jihao Liu</span>,
                  <span class="author">Peng Gao</span>,
                  <span class="author">Qian Zhang</span>,
                  <span class="author">Shiming Xiang</span>
                  <span class="author">Chunhong Pan</span>,
                </div>
                <div>
                  <span class="venue">ECCV 2020</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/1911.05253.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/jiangzhengkai/LSTS">Code</a></span> 
                </div>
                <br>
                <div>
                  The offsets of sampling locations across videos are treated as parameters and would be learned through 
                  back-propagation guided by bounding box and classification loss.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/arxiv2019.jpg" alt="cbgs"></div>
              <div class="text">
                <div class="title">Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection</div>
                <div class="authors">
                  <span class="author">Benjin Zhu</span>,
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Xiangxin Zhou</span>,
                  <span class="author">Zeming Li</span>,
                  <span class="author">Gang Yu</span>
                </div>
                <div>
                  <span class="venue">Arxiv 2019</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/1908.09492.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/poodarchu/Det3D">Code</a></span>
                </div>
                <br>
                <div>
                    NuScenes winner of WAD 2019 3D Object Detection Challenges at CVPR 2019 workshop.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/cvpr2019.jpg" alt="cross-modality"></div>
              <div class="text">
                <div class="title">Dynamic Fusion with Intra and Inter-Modality Attention Flow for Visual Question Answering</div>
                <div class="authors">
                  <span class="author">Peng Gao</span>,
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Haoxuan You</span>,
                  <span class="author">Pan Lu</span>,
                  <span class="author">Steven CH Hoi</span>,
                  <span class="author">Xiaogang Wang</span>,
                  <span class="author">Hongsheng Li</span>
                </div>
                <div>
                  <span class="venue">CVPR 2019</span>
                  <span class="tag"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Dynamic_Fusion_With_Intra-_and_Inter-Modality_Attention_Flow_for_Visual_CVPR_2019_paper.html">Paper</a></span>
                </div>
                <div>
                  <span class="highlight">Oral Presentation</span>
                </div>
                <br>
                <div>
                  Cross modality self-attention was proposed to capture the high-level interactions between language and vision domains.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/aaai2019.jpg" alt="lwdn"></div>
              <div class="text">
                <div class="title">Video Object Detection with Locally-Weightd Deformable Neighboors</div>
                <div class="authors">
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Peng Gao</span>,
                  <span class="author">Chaoxu Guo</span>,
                  <span class="author">Qian Zhang</span>,
                  <span class="author">Shiming Xiang</span>,
                  <span class="author">Chunhong Pan</span>
                </div>
                <div>
                  <span class="venue">AAAI 2019</span>
                  <span class="tag"><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4871">Paper</a></span>
                </div>
                <br>
                <div>
                  Locally-weighted deformable neighbors module was proposed to model motion for video object detection without utilizing time-consuming optical flow extraction networks.
                </div>
              </div>
            </div>
        
        <div class="content anchor" id="education">
          <div class="text" style="z-index:1;position:relative">
            <h3 style="margin-bottom:0em">
              Education
            </h3>
          </div>
          <ul>
            <li>[2017.09-2020.07] &nbsp; National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences.
            <li>[2013.09-2017.07] &nbsp; Department of Automation, Northeastern University.
          </ul>  
        
        <div class="content anchor" id="academic competitions">
          <div class="text" style="z-index:1;position:relative">
            <h3 style="margin-bottom:0em">
              Academic Competitions
            </h3>
          </div>
          <ul>
            <li>1st at the <a href="http://competition.baai.ac.cn/c/36/rank/timeline/72?sourceType=public">Objects 365 Challenge, 2020</a>. </li>
            <li>1st at the <a href="https://www.nuscenes.org/object-detection?externalData=all&mapData=all&modalities=Any">NuScenes 3D Detection of CVPR WAD workshop, 2019</a> </li>
            <li>3rd at the <a href="http://cocodataset.org/#home">Instance Segmentation of ECCV COCO Workshop, 2018</a> </li> 
          </ul>
        
        <div class="content anchor" id="honors & awards">
          <div class="text" style="z-index:1;position:relative">
            <h3 style="margin-bottom:0em">
              Honors & Awards
            </h3>
          </div>
          <ul>
            <li>Excellent Student, <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>, 2019, 2020.</li>
            <li>Special Freshman Scholarship, <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>, 2017.</li>
            <li>National Scholarship, 2014, 2015, 2016.</li>
            <li>First Prize of <a href="http://www.cmathc.cn/">Chinese Mathematics Competitions (CMC)</a>, 2014, 2015.</li> 
            <li>Second Prize of <a href="http://www.cmathc.cn/">Final Chinese Mathematics Competitions (CMC), Wuhan, China</a>, 2014.</li>   
            <li>Third Prize of <a href="http://www.cmathc.cn/">Final Chinese Mathematics Competitions (CMC), Fuzhou, China</a>, 2015.</li>   
            <li>First Class Scholarship of NEU, 2014, 2016</li>
            <li>Second Prize of <a href="http://en.mcm.edu.cn/">China Undergraduate Mathematical Contest in Modeling (CUMCM)</a>, 2015.</li>
          </ul>

        <div class="content anchor" id="professional activities">
          <div class="text" style="z-index:1;position:relative">
            <h3 style="margin-bottom:0em">
              Professional Activities
            </h3>
          </div>
          <ul>
            <li>Conference Reviewer:</li>
              <p>
                IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 2021. <br/>
                Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI) 2021. <br/>
              </p>
            <li>Journal Reviewer:</li>
              <p>
                International Journal of Computer Vision (IJCV). <br />
              </p>
          </ul>
          
        <div class="content anchor">
          <div class="text" style="z-index:1;position:relative">
            Total Visitors:<a href='https://www.counter12.com'><img src='https://www.counter12.com/img-A7zC51Bwy371Wx73-3.gif' border='0' alt='web counter free'></a>
            <script type='text/javascript' src='https://www.counter12.com/ad.js?id=A7zC51Bwy371Wx73'></script>
          </div>  
        </div>
      
      </div>  <!-- content -->
    </div> <!-- container -->
  </div> <!-- outer container -->
<script>showPubs(0);</script>
<script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script></body>
</html>
