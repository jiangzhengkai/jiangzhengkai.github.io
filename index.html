<head>
    <title>Zhengkai Jiang</title>
    <meta name="author" content="Zhengkai Jiang">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Zhengkai Jiang">
    <meta property="og:description" content="Researcher, Tencent">
    <meta property="og:image" content="files/me.png">
    <meta property="og:url" content="https://jiangzhengkai.github.io/">
    <link rel="stylesheet" href="style.css">
</head>

<div class="header noselect">
    <div class="content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Zhengkai JIANG</h1>
            </div>
            <div class="header-subtitle">
                Staff Researcher
            </div>
            <div class="header-links">
                <a class="btn" href="#contact">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?hl=en&user=ooBQi6EAAAAJ&view_op=list_works">Google Scholar</a> /
                <a class="btn" href="https://github.com/jiangzhengkai">GitHub</a> /
                <a class="btn" href="https://www.linkedin.com/in/zhengkaijiang/">LinkedIn</a> 
            </div>
        </div>
    </div>
</div>
<div class="content" style="padding-bottom: 48px;">
    <div>
        <p>
            I focus on post-training, and model efficiency of generative models for Tencent Hunyuan.  
            <br>
        </p>
    </div>

    <div>
        <h2 class="noselect">Publications</h2>

        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/abs/2411.15277"> Foundation Cures Personalization: Recovering Facial Personalized Models' Prompt Consistency</a></br>
                <div class="author">
                    Yiyang Cai, <span class="bold">Zhengkai Jiang</span>, Yulong Liu, Chunyang Jiang, Wei Xue, Wenhan Luo, Yike Guo<br/>
                </div>
                <div class="btn">
                    Arxiv 2024 / <a class="btn" href="https://arxiv.org/abs/2411.15277">Paper</a> 
                </div>
        </div>

        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/pdf/2409.11367"> OSV: One Step is Enough for High-Quality Image to Video Generation</a></br>
                <div class="author">
                    Xiaofeng Mao*, <span class="bold">Zhengkai Jiang*</span>, Fuyun Wang*, Jiangning Zhang, Hao Chen, Mingmin Chi, Yabiao Wang, Wenhan Luo<br/>
                </div>
                <div class="btn">
                    CVPR 2025 / <a class="btn" href="https://arxiv.org/pdf/2409.11367">Paper</a> 
                </div>
        </div>
        
        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/pdf/2405.18156">VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation</a><br/>
                <div class="author"> 
                    Qilin Wang*, <span class="bold">Zhengkai Jiang*</span>, Chengming Xu, Jiangning Zhang, Yabiao Wang, Xinyi Zhang, Yun Cao, Weijian Cao, Chengjie Wang, Yanwei Fu<br/>
                </div>
                <div class="btn">
                    Arxiv 2024 / <a class="btn" href="https://arxiv.org/pdf/2405.18156">Paper</a> / <a class="btn" href="https://kelu007.github.io/vivid-pose/">Code</a>
                </div>
        </div>
        
        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/pdf/2305.03048.pdf">Personalize Segment Anything Model with One Shot</a><br/>
                <div class="author"> 
                    Renrui Zhang, <span class="bold">Zhengkai Jiang</span>, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Yu Qiao, Peng Gao, Hongsheng Li<br/>
                </div>
                <div class="btn">
                    ICLR 2024 / <a class="btn" href="https://arxiv.org/pdf/2305.03048.pdf">Paper</a> / <a class="btn" href="https://github.com/ZrrSkywalker/Personalize-SAM">Code</a>
                </div>
        </div>

        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/pdf/2408.03312">MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation</a><br/>
                <div class="author"> 
                    Xiaofeng Mao*, <span class="bold">Zhengkai Jiang</span>*, Qilin Wang, Chencan Fu, Jiangning Zhang, Jiafu Wu, Yabiao Wang, Chengjie Wang, Wei Li, Mingnin Chi<br/>
                </div>
                <div class="btn">
                    ACM MM 2024 / <a class="btn" href="https://arxiv.org/pdf/2408.03312">Paper</a>
                </div>
        </div>

        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/abs/2403.11561">Learning Unified Reference Representation for Unsupervised Multi-class Anomaly Detection</a><br/>
                <div class="author"> 
                    Liren He*, <span class="bold">Zhengkai Jiang</span>*, Jinlong Peng, Liang Liu, Qiangang Du, Xiaobin Hu, Wenbing Zhu, Mingmin Chi, Yabiao Wang, Chengjie Wang<br/>
                </div>
                <div class="btn">
                    ECCV 2024 / <a class="btn" href="https://arxiv.org/abs/2403.11561">Paper</a>
                </div>
        </div>
        
        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/abs/2312.09595">Density Matters: Improved Core-set for Active Domain Adaptive Segmentation</a><br/>
                <div class="author"> 
                    Shizhan Liu*, <span class="bold">Zhengkai Jiang</span>*, Yuxi Li, Jinlong Peng, Yabiao Wang, Weiyao Lin<br/>
                </div>
                <div class="btn">
                    AAAI 2024 / <a class="btn" href="https://arxiv.org/abs/2312.09595">Paper</a>
                </div>
        </div>
        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/pdf/2207.06654.pdf">Prototypical Contrast Adaptation for Domain Adaptive Segmentation</a><br/>
                <div class="author"> 
                    <span class="bold">Zhengkai Jiang</span>, Yuxi Li, Ceyuan Yang, Peng Gao, Yabiao Wang, Ying Tai, Chengjie Wang<br/>
                </div>
                <div class="btn">
                    ECCV 2022 / <a class="btn" href="https://arxiv.org/pdf/2207.06654.pdf">Paper</a>  / <a class="btn" href="https://github.com/jiangzhengkai/ProCA">Code</a>
                </div>
        </div>
        
        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/pdf/2105.11237.pdf">SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</a><br/>
                <div class="author"> 
                    Jinlong Peng*, <span class="bold">Zhengkai Jiang</span>*, Yueyang Gu, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Weiyao Lin<br/>
                </div>
                <div class="btn">
                    IJCAI 2021 / <a class="btn" href="https://arxiv.org/pdf/2105.11237.pdf">Paper</a> 
                </div>
        </div>
        <div class="publication row clearfix">
            <a class="publication-title" href="https://arxiv.org/pdf/1911.05253.pdf">Learning Where to Focus for Efficient Video Object Detection</a><br/>
                <div class="author"> 
                    <span class="bold">Zhengkai Jiang</span>, Yu Liu, Ceyuan Yang, Jihao Liu, Peng Gao, Qian Zhang, Shiming Xiang, Chunhong Pan<br/>
                </div>
                <div class="btn">
                    ECCV 2020 / <a class="btn" href="https://jiangzhengkai.github.io/LSTS/">Project Page</a> / <a class="btn btn-red" href="https://arxiv.org/pdf/1911.05253.pdf">Paper</a> / <a class="btn" href="https://github.com/jiangzhengkai/LSTS">Code</a>
                </div>
        </div>
        
        <div class="publication row clearfix">
            <a class="publication-title" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Dynamic_Fusion_With_Intra-_and_Inter-Modality_Attention_Flow_for_Visual_CVPR_2019_paper.pdf">Dynamic Fusion with Intra-and Inter-Modality Attention Flow for Visual Question Answering</a><br/>
                <div class="author">     
                    Peng Gao, <span class="bold">Zhengkai Jiang</span>, Haoxuan You, Pan Lu, Steven CH Hoi, Xiaogang Wang, Hongsheng Li<br/>
                </div>
                <div class="btn">
                    CVPR 2019 / <a class="btn" href="https://openaccess.thecvf.com/content_CVPR_2019/papers/Gao_Dynamic_Fusion_With_Intra-_and_Inter-Modality_Attention_Flow_for_Visual_CVPR_2019_paper.pdf">Paper</a>
                </div>
        </div>
        <div class="publication row clearfix">
            <a class="publication-title" href="https://ojs.aaai.org//index.php/AAAI/article/view/4871">Video Object Detection with Locally-Weightd Deformable Neighboors</a><br/>
                <div class="author">     
                    <span class="bold">Zhengkai Jiang</span>, Peng Gao, Chaoxu Guo, Qian Zhang, Shiming Xiang, Chunhong Pan<br/>
                </div>
                <div class="btn">
                    AAAI 2019 / <a class="btn" href="https://ojs.aaai.org//index.php/AAAI/article/view/4871">Paper</a>
                </div>
        </div>
    </div>
    <div class="noselect">
        <a id="Honor"></a>
        <h2>Honors and Awards</h2>
        <p> 
            <li>1st of the Objects 365 Challenge, 2020</li>
            <li>1st of the NuScenes 3D Detection of CVPR WAD workshop, 2019</li>
            <li>Outstanding graduates of Liaoning Province, 2017</li> 
            <li>1st of the Chinese Mathematics Competitions (1/6000+ in Liaoning Province), 2015, 2016</li> 
            <li>National Scholarship (Top 0.2% students in China), 2014, 2015, 2016</li>
        </p>
    </div>
    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        You are very welcome to contact me. My email is <span class="bold">kaikaijiang.jzk</span> [at] <span class="bold">gmail</span> .com or  <span class="bold">zkjiang</span> [at] <span class="bold">ust</span> .hk
    </div>
</div>
