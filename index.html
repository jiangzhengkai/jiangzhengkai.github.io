<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async="" src="./files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-WYV42QGLZ8');
  </script>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ”¥</text></svg>">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
  <title>Zhengkai Jiang</title>
  <link rel="stylesheet" href="./files/font.css">
  <link rel="stylesheet" href="./files/main.css">
  <script src="./files/main.js"></script>
  <script src="./files/scroll.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.993.0" data-gr-ext-installed="">
    <div class="outercontainer">
      <script src="./files/header.js"></script>
      <header>
        <div class="container header">
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#home">Home</a></div>
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#publications">Publications</a></div>       
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#honors">Honors</a></div>   
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#services">Services</a></div>        
        </div>
      </header>
      <div class="container body">
        <div class="content heading anchor" id="home">
          <div class="text info">
            <h1>Zhengkai JIANG (è’‹æ­£é”´)</h1>
            <p>
            </p>
            <div>Researcher</div>
            <div>Tencent Youtu Lab</div>
            <div>Email:&nbsp;zhengkjiang [at] tencent (dot) com</div>
            <p>
            <span><a href="https://scholar.google.com/citations?user=ooBQi6EAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a></span> / 
            <span><a href="https://github.com/jiangzhengkai">Github</a></span> / 
            <span><a href="https://www.zhihu.com/people/zhengkaijiang">Zhihu</a></span> /
            <span><a href="https://twitter.com/jiang_zhengkai">Twitter</a></span> / 
            <span><a href="https://www.linkedin.com/in/zhengkaijiang/">LinkedIn</a></span>                 
            </p>
            <p>
            </p>
          </div>
          <div class="img"><img class="avatar" src="./imgs/me.jpg" alt="Photo"></div>
          <div class="text info">
            <p>I am currently a Researcher at <a href="https://ai.qq.com/hr/youtu.shtml">Tencent Youtu Lab</a>. 
               Previously, I obtained the master's degree at 2020 from <a href="http://www.nlpr.ia.ac.cn">National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</a>. 
               I received the bachelor's degree at 2017 from the Department of Automation, <a href="http://english.neu.edu.cn/">Northeastern University</a> with honors.
               <br><br>
               Research Interest:
               <ul style="padding-bottom:1em">
                <li> CNN/Transformer Backbone Design</li>
                <li> Object Detection, Instance Segmentation and Semantic Segmentation (Image, Video, and 3D)</li>
                <li> Data/Label Efficient Learning, Including Domain Adaptation, Self-Supervised Learning, and Semi-Supervised Learning</li>
                <li> Open-World Recognition, Detection, and Segmentation</li>
              </ul>
            </p>
          </div>
        </div>

        <div class="content" style="z-index:1;position:relative">
          <div class="text">
            <h2 style="margin-bottom:.5em">News</h2>
            <ul style="padding-bottom:1em">
              <li><strong>[04/2021]</strong> One paper is accepted by <a href="https://www.ijcai.org/" target="_blank">IJCAI 2021</a>.</li> 
              <li><strong>[07/2020]</strong> One paper is accepted by <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a>.</li>
              <li><strong>[03/2019]</strong> One paper is accepted by <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a>. </li>
              <li><strong>[11/2018]</strong> One paper is accepted by <a href="https://www.aaai.org/" target="_blank">AAAI 2019</a>. </li>              
            </ul>
          </div>
        </div>

        <div class="content anchor" id="publications">
          <div class="text" style="z-index:1;position:relative">
            <h2 style="margin-bottom:0em">
              Publications
            </h2>
          </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>
            
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/ijcai2021.jpg" alt="siamrcr"></div>
              <div class="text">
                <div class="title">SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</div> 
                <div class="authors">
                  <span class="author">Jinlong Peng*</span>,
                  <span class="author jw">Zhengkai Jiang*</span>,
                  <span class="author">Yueyang Gu*</span>,
                  <span class="author">Yang Wu</span>, 
                  <span class="author">Yabiao Wang</span>, 
                  <span class="author">Ying Tai</span>, 
                  <span class="author">Chengjie Wang</span>,
                  <span class="author">Weiyao Lin</span>
                </div>
                <div>
                  <span class="venue">IJCAI 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2105.11237.pdf">Paper</a></span>                   
                </div>
                <div>
                  Reciprocal classification and regression was proposed for visual object tracking.
                </div>
              </div>
            </div>
            
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/eccv2020.jpg" alt="lsts"></div>
              <div class="text">
                <div class="title">Learning Where to Focus for Efficient Video Object Detection</div> 
                <div class="authors">
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Yu Liu</span>,
                  <span class="author">Ceyuan Yang</span>,
                  <span class="author">Jihao Liu</span>,
                  <span class="author">Peng Gao</span>,
                  <span class="author">Qian Zhang</span>,
                  <span class="author">Shiming Xiang</span>
                  <span class="author">Chunhong Pan</span>
                </div>
                <div>
                  <span class="venue">ECCV 2020</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/1911.05253.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/jiangzhengkai/LSTS">Code</a></span> 
                </div>
                <div>
                  The offsets of sampling locations across videos are treated as parameters and would be learned through 
                  back-propagation guided by bounding box and classification loss.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/cvpr2019.jpg" alt="cross-modality"></div>
              <div class="text">
                <div class="title">Dynamic Fusion with Intra and Inter-Modality Attention Flow for Visual Question Answering</div>
                <div class="authors">
                  <span class="author">Peng Gao</span>,
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Haoxuan You</span>,
                  <span class="author">Pan Lu</span>,
                  <span class="author">Steven CH Hoi</span>,
                  <span class="author">Xiaogang Wang</span>,
                  <span class="author">Hongsheng Li</span>
                </div>
                <div>
                  <span class="venue">CVPR 2019</span> /
                  <span class="tag"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Dynamic_Fusion_With_Intra-_and_Inter-Modality_Attention_Flow_for_Visual_CVPR_2019_paper.html">Paper</a></span>
                </div>
                <div>
                  <span class="highlight">Oral Presentation</span>
                </div>
                <div>
                  Cross modality self-attention was proposed to capture the high-level interactions between language and vision domains.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/aaai2019.jpg" alt="lwdn"></div>
              <div class="text">
                <div class="title">Video Object Detection with Locally-Weightd Deformable Neighboors</div>
                <div class="authors">
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Peng Gao</span>,
                  <span class="author">Chaoxu Guo</span>,
                  <span class="author">Qian Zhang</span>,
                  <span class="author">Shiming Xiang</span>,
                  <span class="author">Chunhong Pan</span>
                </div>
                <div>
                  <span class="venue">AAAI 2019</span> /
                  <span class="tag"><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4871">Paper</a></span>
                </div>
                <div>
                  Locally-weighted deformable neighbors module was proposed to model motion for video object detection without utilizing time-consuming optical flow extraction networks.
                </div>
              </div>
            </div>
       
        <div class="content anchor" id="honors">
          <div class="text" style="z-index:1;position:relative">
            <h2 style="margin-bottom:0em">
              Honors
            </h2>
          </div>
          <ul>
            <li>Outstanding Staff Award, Tencent, 2021</li>
            <li>Excellent Student, <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>, 2019, 2020</li>
            <li>Special Freshman Scholarship, <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>, 2017</li>
            <li>Outstanding Graduates of NEU, 2017</li>
            <li>Outstanding Graduates of Liaoning Province, 2017</li>
            <li>First Prize of <a href="http://www.cmathc.cn/">Chinese Mathematics Competitions (CMC)</a>, 2014, 2015</li> 
            <li>Second Prize of <a href="http://www.cmathc.cn/">Final Chinese Mathematics Competitions (CMC), Wuhan, China</a>, 2014</li>   
            <li>Third Prize of <a href="http://www.cmathc.cn/">Final Chinese Mathematics Competitions (CMC), Fuzhou, China</a>, 2015</li>   
            <li>Second Prize of <a href="http://en.mcm.edu.cn/">China Undergraduate Mathematical Contest in Modeling (CUMCM)</a>, 2015</li>
            <li>First Class Scholarship of NEU, 2014, 2016</li>
            <li>National Scholarship, 2014, 2015, 2016</li>          
          </ul>

        <div class="content anchor" id="services">
          <div class="text" style="z-index:1;position:relative">
            <h2 style="margin-bottom:0em">
              Services
            </h2>
          </div>
          <ul>
            <li>Conference Reviewer:</li>
                International Conference on Machine Learning (ICML), 2022 <br/>
                International Conference on Learning Representations (ICLR), 2022 <br/>
                Conference on Neural Information Processing Systems (NeurIPS), 2021 <br/>
                European Conference on Computer Vision (ECCV), 2022 <br/>
                International Conference on Computer Vision (ICCV), 2021 <br/>                
                IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, 2022<br/>
                AAAI Conference on Artificial Intelligence (AAAI), 2021<br/>
            <li>Journal Reviewer:</li>
                IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)<br/>
                International Journal of Computer Vision (IJCV)<br/>
                Transactions on Machine Learning Research (TMLR)<br/>   
          </ul>
      </div>  <!-- content -->
    </div> <!-- container -->
  </div> <!-- outer container -->
  <script>showPubs(0);</script>
  <script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
</body>
</html>
