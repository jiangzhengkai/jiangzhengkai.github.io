<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async="" src="./files/js"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-WYV42QGLZ8');
  </script>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0">
  <title>Zhengkai Jiang</title>
  <link rel="stylesheet" href="./files/font.css">
  <link rel="stylesheet" href="./files/main.css">
  <script src="./files/main.js"></script>
  <script src="./files/scroll.js"></script>
</head>

<body data-new-gr-c-s-check-loaded="14.993.0" data-gr-ext-installed="">
    <div class="outercontainer">
      <script src="./files/header.js"></script>
      <header>
        <div class="container header">
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#home">Home</a></div>
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#publications">Publications</a></div>       
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#honors">Honors</a></div>   
          <div class="ftheader text"><a href="https://jiangzhengkai.github.io/#services">Services</a></div>        
        </div>
      </header>
      <div class="container body">
        <div class="content heading anchor" id="home">
          <div class="text info">
            <h1>Zhengkai Jiang(蒋正锴)/h1>
            <p>
            </p>
            <div>Researcher</div>
            <div>Tencent Youtu Lab</div>
            <div>Email:&nbsp;zhengkjiang [at] tencent (dot) com</div>
            <p>
            <span><a href="https://scholar.google.com/citations?user=ooBQi6EAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a></span> / 
            <span><a href="https://github.com/jiangzhengkai">Github</a></span> / 
            <span><a href="https://www.zhihu.com/people/zhengkaijiang">Zhihu</a></span> /
            <span><a href="https://twitter.com/jiang_zhengkai">Twitter</a></span> / 
            <span><a href="https://www.linkedin.com/in/zhengkaijiang/">LinkedIn</a></span>                 
            </p>
            <p>
            </p>
          </div>
          <div class="img"><img class="avatar" src="./imgs/me.jpg" alt="Photo"></div>
          <div class="text info">
            <p>I am currently a Researcher at <a href="https://ai.qq.com/hr/youtu.shtml">Tencent Youtu Lab</a>. My research interests are in computer vision and deep learning. Currently, I'm working on data-efficient learning for object detection, segmentation and tracking.
               <br><br>
               Previously, I obtained the master's degree at 2020 from <a href="http://www.nlpr.ia.ac.cn">National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</a>. 
               I received the bachelor's degree at 2017 from the Department of Automation, <a href="http://english.neu.edu.cn/">Northeastern University</a> with honors.
               <br><br>
               <strong>I am looking for highly motivated research interns working on object detection, instance segmentation and data-efficient learning based in Shanghai.</strong>
            </p>
          </div>
        </div>

        <div class="content" style="z-index:1;position:relative">
          <div class="text">
            <h2 style="margin-bottom:.5em">News</h2>
            <ul style="padding-bottom:1em">
              <li><strong>[12/2021]</strong> One paper is accepted by <a href="http://www.aaai.org/" target="_blank">AAAI 2022</a>.</li>               
              <li><strong>[07/2021]</strong> One paper is accepted by <a href="http://iccv2021.thecvf.com/home/" target="_blank">ICCV 2021</a>.</li> 
              <li><strong>[04/2021]</strong> One paper is accepted by <a href="https://www.ijcai.org/" target="_blank">IJCAI 2021</a>.</li> 
              <li><strong>[09/2020]</strong> Two papers are accepted by <a href="https://nips.cc/" target="_blank">NeurIPS 2020</a>.</li>
              <li><strong>[07/2020]</strong> One paper is accepted by <a href="https://eccv2020.eu/" target="_blank">ECCV 2020</a>.</li>
              <li><strong>[03/2019]</strong> One paper is accepted by <a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR 2019</a>. </li>
              <li><strong>[11/2018]</strong> One paper is accepted by <a href="https://www.aaai.org/" target="_blank">AAAI 2019</a>. </li>              
            </ul>
          </div>
        </div>

        <div class="content anchor" id="publications">
          <div class="text" style="z-index:1;position:relative">
            <h2 style="margin-bottom:0em">
              Publications
            </h2>
          </div>
          
          <div id="pubs">
            <div class="text anchor">&nbsp;</div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/iccv2021.jpg" alt="p2pnet"></div>
              <div class="text">
                <div class="title">Rethinking Counting and Localization in Crowds: A Purely Point-Based Framework</div> 
                <div class="authors">
                  <span class="author">Qingyu Song*</span>,
                  <span class="author">Changan Wang*</span>,
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Yabiao Wang</span>, 
                  <span class="author">Ying Tai</span>, 
                  <span class="author">Chengjie Wang</span>, 
                  <span class="author">Jilin Li</span>,
                  <span class="author">Feiyue Huang</span>,
                  <span class="author">Yang Wu</span>
                </div>
                <div>
                  <span class="venue">ICCV 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2107.12746.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/TencentYoutuResearch/CrowdCounting-P2PNet">Code</a></span>                  
                </div>
                <div>
                  <span class="highlight">Oral Presentation</span>
                </div>
                <br>
                <div>
                  The first point-based framework for crowd counting and localization.
                </div>
              </div>
            </div>
            
            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/ijcai2021.jpg" alt="siamrcr"></div>
              <div class="text">
                <div class="title">SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking</div> 
                <div class="authors">
                  <span class="author">Jinlong Peng*</span>,
                  <span class="author jw">Zhengkai Jiang*</span>,
                  <span class="author">Yueyang Gu*</span>,
                  <span class="author">Yang Wu</span>, 
                  <span class="author">Yabiao Wang</span>, 
                  <span class="author">Ying Tai</span>, 
                  <span class="author">Chengjie Wang</span>,
                  <span class="author">Weiyao Lin</span>
                </div>
                <div>
                  <span class="venue">IJCAI 2021</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2105.11237.pdf">Paper</a></span>                   
                </div>
                <br>
                <div>
                  Reciprocal classification and regression was proposed for visual object tracking.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/autoassign2020.jpg" alt="autoassign"></div>
              <div class="text">
                <div class="title">AutoAssign: Differentiable Label Assignment for Dense Object Detection</div> 
                <div class="authors">
                  <span class="author">Benjin Zhu</span>,
                  <span class="author">Jianfeng Wang</span>,
                  <span class="author jw">Zhengkai Jiang</span>, 
                  <span class="author">Fuhang Zong</span>, 
                  <span class="author">Songtao Liu</span>, 
                  <span class="author">Zeming Li</span>,
                  <span class="author">Jian Sun</span>
                </div>
                <div>
                  <span class="venue">Arxiv 2020</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/2007.03496.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://megvii-basedetection/AutoAssign">Code</a></span>
                </div>
                <br>
                <div>
                  Performing differentiable label assignment for dense object detection.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/dynamichead.jpg" alt="dynamichead"></div>
              <div class="text">
                <div class="title">Fine-Grained Dynamic Head for Object Detection</div> 
                <div class="authors">
                  <span class="author">Lin Song</span>,
                  <span class="author">Yanwei Li</span>,
                  <span class="author jw">Zhengkai Jiang</span>, 
                  <span class="author">Zeming Li</span>, 
                  <span class="author">Hongbin Sun</span>,
                  <span class="author">Jian Sun</span>,
                  <span class="author">Nanning Zheng</span>
                </div>
                <div>
                  <span class="venue">NeurIPS 2020</span> /
                  <span class="tag"><a href="https://papers.nips.cc/paper/2020/file/7f6caf1f0ba788cd7953d817724c2b6e-Paper.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/StevenGrove/DynamicHead">Code</a></span>
                </div>
                <br>
                <div>
                  A fine-grained dynamic head is proposed to conditionally select a pixel-level combination of FPN features from different scales.
                </div>
              </div>
            </div>


            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/ltf-v2.jpg" alt="ltf-v2"></div>
              <div class="text">
                <div class="title">Rethinking Learnable Tree Filter for Generic Feature Transform</div> 
                <div class="authors">
                  <span class="author">Lin Song</span>,
                  <span class="author">Yanwei Li</span>,
                  <span class="author jw">Zhengkai Jiang</span>, 
                  <span class="author">Zeming Li</span>, 
                  <span class="author">Xiangyu Zhang</span>, 
                  <span class="author">Hongbin Sun</span>,
                  <span class="author">Jian Sun</span>,
                  <span class="author">Nanning Zheng</span>
                </div>
                <div>
                  <span class="venue">NeurIPS 2020</span> /
                  <span class="tag"><a href="https://papers.nips.cc/paper/2020/file/2952351097998ac1240cb2ab7333a3d2-Paper.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/StevenGrove/LearnableTreeFilterV2">Code</a></span>
                </div>
                <br>
                <div>
                  Performing differentiable tree-filter with a learnable unary term for generic feature transform.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/eccv2020.jpg" alt="lsts"></div>
              <div class="text">
                <div class="title">Learning Where to Focus for Efficient Video Object Detection</div> 
                <div class="authors">
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Yu Liu</span>,
                  <span class="author">Ceyuan Yang</span>,
                  <span class="author">Jihao Liu</span>,
                  <span class="author">Peng Gao</span>,
                  <span class="author">Qian Zhang</span>,
                  <span class="author">Shiming Xiang</span>
                  <span class="author">Chunhong Pan</span>
                </div>
                <div>
                  <span class="venue">ECCV 2020</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/1911.05253.pdf">Paper</a></span> / 
                  <span class="tag"><a href="https://github.com/jiangzhengkai/LSTS">Code</a></span> 
                </div>
                <br>
                <div>
                  The offsets of sampling locations across videos are treated as parameters and would be learned through 
                  back-propagation guided by bounding box and classification loss.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/arxiv2019.jpg" alt="cbgs"></div>
              <div class="text">
                <div class="title">Class-balanced Grouping and Sampling for Point Cloud 3D Object Detection</div>
                <div class="authors">
                  <span class="author">Benjin Zhu</span>,
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Xiangxin Zhou</span>,
                  <span class="author">Zeming Li</span>,
                  <span class="author">Gang Yu</span>
                </div>
                <div>
                  <span class="venue">Arxiv 2019</span> /
                  <span class="tag"><a href="https://arxiv.org/pdf/1908.09492.pdf">Paper</a></span> /
                  <span class="tag"><a href="https://github.com/poodarchu/Det3D">Code</a></span>
                </div>
                <br>
                <div>
                    NuScenes winner of WAD 2019 3D Object Detection Challenges at CVPR 2019 workshop.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/cvpr2019.jpg" alt="cross-modality"></div>
              <div class="text">
                <div class="title">Dynamic Fusion with Intra and Inter-Modality Attention Flow for Visual Question Answering</div>
                <div class="authors">
                  <span class="author">Peng Gao</span>,
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Haoxuan You</span>,
                  <span class="author">Pan Lu</span>,
                  <span class="author">Steven CH Hoi</span>,
                  <span class="author">Xiaogang Wang</span>,
                  <span class="author">Hongsheng Li</span>
                </div>
                <div>
                  <span class="venue">CVPR 2019</span> /
                  <span class="tag"><a href="http://openaccess.thecvf.com/content_CVPR_2019/html/Gao_Dynamic_Fusion_With_Intra-_and_Inter-Modality_Attention_Flow_for_Visual_CVPR_2019_paper.html">Paper</a></span>
                </div>
                <div>
                  <span class="highlight">Oral Presentation</span>
                </div>
                <br>
                <div>
                  Cross modality self-attention was proposed to capture the high-level interactions between language and vision domains.
                </div>
              </div>
            </div>

            <div class="publication">
              <div class="img"><img class="img_responsive" src="./imgs/aaai2019.jpg" alt="lwdn"></div>
              <div class="text">
                <div class="title">Video Object Detection with Locally-Weightd Deformable Neighboors</div>
                <div class="authors">
                  <span class="author jw">Zhengkai Jiang</span>,
                  <span class="author">Peng Gao</span>,
                  <span class="author">Chaoxu Guo</span>,
                  <span class="author">Qian Zhang</span>,
                  <span class="author">Shiming Xiang</span>,
                  <span class="author">Chunhong Pan</span>
                </div>
                <div>
                  <span class="venue">AAAI 2019</span> /
                  <span class="tag"><a href="https://www.aaai.org/ojs/index.php/AAAI/article/view/4871">Paper</a></span>
                </div>
                <br>
                <div>
                  Locally-weighted deformable neighbors module was proposed to model motion for video object detection without utilizing time-consuming optical flow extraction networks.
                </div>
              </div>
            </div>
       
        <div class="content anchor" id="honors">
          <div class="text" style="z-index:1;position:relative">
            <h2 style="margin-bottom:0em">
              Honors
            </h2>
          </div>
          <ul>
            <li>Excellent Student, <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>, 2019, 2020</li>
            <li>Special Freshman Scholarship, <a href="http://english.ia.cas.cn/">Institute of Automation, Chinese Academy of Sciences</a>, 2017</li>
            <li>Outstanding Graduates of NEU, 2017</li>
            <li>Outstanding Graduates of Liaoning Province, 2017</li>
            <li>First Prize of <a href="http://www.cmathc.cn/">Chinese Mathematics Competitions (CMC)</a>, 2014, 2015</li> 
            <li>Second Prize of <a href="http://www.cmathc.cn/">Final Chinese Mathematics Competitions (CMC), Wuhan, China</a>, 2014</li>   
            <li>Third Prize of <a href="http://www.cmathc.cn/">Final Chinese Mathematics Competitions (CMC), Fuzhou, China</a>, 2015</li>   
            <li>Second Prize of <a href="http://en.mcm.edu.cn/">China Undergraduate Mathematical Contest in Modeling (CUMCM)</a>, 2015</li>
            <li>First Class Scholarship of NEU, 2014, 2016</li>
            <li>National Scholarship, 2014, 2015, 2016</li>          
          </ul>

        <div class="content anchor" id="services">
          <div class="text" style="z-index:1;position:relative">
            <h2 style="margin-bottom:0em">
              Services
            </h2>
          </div>
          <ul>
            <li>Conference Reviewer:</li>
                The International Conference on Learning Representations (ICLR), 2022 <br/>
                Conference on Neural Information Processing Systems (NeurIPS), 2021 <br/>
                International Conference on Computer Vision (ICCV), 2021 <br/>
                IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021, 2022<br/>
                Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI), 2021<br/>
            <li>Journal Reviewer:</li>
                International Journal of Computer Vision (IJCV)<br />
          </ul>
      </div>  <!-- content -->
    </div> <!-- container -->
  </div> <!-- outer container -->
  <script>showPubs(0);</script>
  <script>var scroll = new SmoothScroll('a[href*="#"]', {speed: 1000});</script>
</body>
</html>
